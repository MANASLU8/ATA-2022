Данный проект является результатом выполнения практических заданий курса ИТМО по автоматической обработке текстов (2022).

## Структура проекта

* `assets` - вспомогательные и конфигурационные файлы, не являющиеся исходным кодом.
    * `raw-dataset` - директория с исходными датасетами (добавлена в .gitignore).
    * `test` - генерируемая модулем токенизации директория.
    * `train` - генерируемая модулем токенизации директория c .
* `source` - исходный код.
    * `tokenizer` - модуль, реализующий функциональность токенизации.
    * `typos` - модуль исправления опечаток.
    * `vectorizer` - модуль векторизации текстового представления.
    * `tests` - модульные тесты.
      * `test_tokenizer` - тесты для демонстации разработанной функциональности токенизатора.
      * `test_typos` - тесты для демонстации работы модуля исправления опечаток.
* `README.md` - описание.

## Подготовка окружения
Для корректной работы в директории `../assets/raw-dataset` должны располагаться два исходных датасета `train.csv` и `test.csv`.

## Токенизатор

Для запуска токенизатора следует убедиться, что в директории `raw-dataset` присутствуют файлы `test.csv` и `train.csv` и выполнить следующую команду из директории `tokenizer` проекта
```
python __main__.py
```
Результатом выполнения является набор набор аннотаций в формате tsv в соответствии со следующей структурой:
```
<token_type>    <sentence_2_token_N>    <sentence_2_stem_N>    <sentence_2_lemma_N>
```
Для выполнения лабораторных работ был выбран датасет https://huggingface.co/datasets/ag_news
В директории /assets/annotated-corpus сформированы каталоги train и test, в которых будет размещаться набор файлов с расширением tsv, содержащие аннотации документов, составляющих исходный датасет. 
Каждому документу исходного датасета соответствует отдельный файл, в качестве названия файла используется идентификатор документа. Документы группируются по директориям в соответствии с их разбиением на классы, название класса используется в качестве названия соответствующей директории.

Пример содержимого сгенерированных файлов
```
Companies	compani	Companies
surrenders	surrend	surrender
High-Capacity	high-capac	High-Capacity
ESPN	espn	ESPN
associate	associ	associate
"Music Manifesto"
```
Токены, не являющиеся словами, такие как аббревиатуры, знаки пунктуации, числа, названия в кавычках не участвуют в дальнейшей нормализации и не передаются для стемминга и лемматизации.


Для лемматизации использовался WordNetLemmatizer. Как можно заметить, он успешно привёл слово 'surrenders' к 'surrender', но 'Companies' осталось неизменным. 
Для того, чтобы учесть такие ситуации, следует различать слова в начале предложения от именованных сущностей и приводить слова в начале предложения к строчным буквам. Однако корректное распознавание именованных сущностей и их дифференцирование от слов в начале предложения возможно лишь при учёте семантики и недостижимо при использовании токенизатора только на основе регулярных выражений.


## Исправление опечаток
Для генерации словаря следует выполнить следующую команду из директории `typos` проекта
```
python __main__.py
```
Далее для запуска модуля исправления опечаток следует убедиться, что в директории `raw-dataset` присутствует файл с опечатками `test-corrupted.csv` и выполнить запуск модуля исправления опечаток по алгоритму Хиршберга из директории `typos` проекта
```
python hirshberg.py
```
или по алгоритму, использующему поиск в окрестности префиксного дерева, построенного по словарю, из директории `typos` проекта
```
python tdict.py
```


    model_path = '../assets/w2v-train-model_5_100_5.bin'
    w2v_train('../assets/raw-dataset/train.csv', 10, 100, 5, model_path)
    demonstrate_w2v_and_tfidf_models(model_path, tokens_entries, 2)
    vectorizer = TextVectorizer(tokens_entries)
    vectorizer.get_w2v_embeddings(model_path, "../assets/raw-dataset/test.csv", "../assets/annotated-corpus/test-embeddings.tsv")
## Векторизация
Перед запуском необходимо убедиться в присутствии исходных датасетов `train.csv` и `test.csv`.
Для генерации словаря следует выполнить следующую команду из директории `vectorizer` проекта
```
python __main__.py
```
Программа сгенерирует файл `../assets/vec_dict`, содержащий информацию о словаре токенов с указанием их частот и матрицу "термин-документ". 
Структура файла - набор записей такого вида:
```sh
имя_токена
:путь_до_файла_с_аннотированными_токенами (см. токенизатор)
-количество_вхождений_токена_в_этом_файле
:путь_до_файла_с_аннотированными_токенами_2 (см. токенизатор)
-количество_вхождений_токена_в_этом_файле
...
```
Далее происходит тренировка модели v2w, которая сохранится в `../assets/w2v-train-model_5_100_5.bin`, после чего выполняется демонстрация на примерах, что для семантически близких слов модель генерирует вектора, для которых косинусное расстояние меньше, чем для семантически далеких токенов.
В заключении будет сформирован файл `../assets/test-embeddings.tsv`, содержащий векторные представления документов в `test.csv` файле.

## Тематическое моделирование
Перед запуском необходимо убедиться в присутствии исходных датасетов `train.csv` и `test.csv`, а также сохранённых словарей `../assets/test_vec_dict` и `../assets/vec_dict`.
Для генерации словаря следует выполнить следующую команду из директории `tematic_modeling` проекта
```
python __main__.py
```
Для каждой LDA модели в качетве текстовых параметров установлены следующие значения:
- min_frequency = 4
- iterations = [2, 4, 8]
- topics = [2, 4, 5, 10, 20, 30, 40]

Для каждого эксперимента буду сгенерированы следующие результаты в отдельных файлах:
- топ-10 ключевых слов для каждой темы
- perplexity полученной модели на тестовой выборке
- вероятность принадлежности документов обучающей выборки к той или иной теме, а также документы для каждой темы, у которых данная вероятность наиболее высока

## Классификация текстов
Перед запуском необходимо убедиться в присутствии исходных датасетов `train.csv` и `test.csv`, а также `../assets/test-embeddings.tsv` и `../assets/train-embeddings.tsv`.
Для генерации словаря следует выполнить следующую команду из директории `classification` проекта
```
python __main__.py
```
Программа сгенерирует файлы с обученными моделями SVM для следующих kernel_types: `linear`, `rbf`, `sigmoid`. В качестве набора итераций для каждого типа были выбраны следущие значения: `1000`, `5000`, `10000`, `25000`, `50000`, `75000`.
Лучший результат показала модель `sigmoid` с количеством итераций `50000`, поэтому для неё также будет сгенерирован результат при фиксации размерности вектора на значениях `4` и `25`.

## Запуск тестов

Для системы разработан набор модульных тестов, позволяющих оценить корректность работы программ. Для запуска тестов используется следующая команда, которую необходимо выполнять из корневой директории проекта:

```sh
PYTHONPATH=source python -m unittest
```

Система отображает стандартный отчет о результатах выполненя тестов:

```sh
...
----------------------------------------------------------------------
Ran 7 tests in 0.028s

OK

OK
```
